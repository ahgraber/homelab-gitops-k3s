{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"ansible/","title":"Ansible","text":"<ul> <li>Ansible</li> <li>Call arbitrary playbooks<ul> <li>With ansible</li> <li>With task</li> </ul> </li> <li>Send arbitrary commands<ul> <li>With ansible</li> <li>With task</li> </ul> </li> </ul>"},{"location":"ansible/#call-arbitrary-playbooks","title":"Call arbitrary playbooks","text":""},{"location":"ansible/#with-ansible","title":"With ansible","text":"<pre><code># paths assume running from /ansible dir\ncd ./ansible\nansible-playbook -i &lt;path/to/inventory&gt; -l &lt;groupname&gt; &lt;path/to/playbook&gt; --become\n# e.g.\n# &gt; ansible-playbook -i ./inventory -l ubuntu ./playbooks/cluster-reboot.yaml --become\n</code></pre>"},{"location":"ansible/#with-task","title":"With task","text":"<pre><code>task ansible:run group='kubernetes' playbook='k3s-reboot' -- '--become'\n</code></pre>"},{"location":"ansible/#send-arbitrary-commands","title":"Send arbitrary commands","text":""},{"location":"ansible/#with-ansible_1","title":"With ansible","text":"<pre><code># paths assume running from /ansible dir\ncd ./ansible\nansible -i &lt;path/to/inventory&gt; -l &lt;groupname&gt; -m ansible.builtin.shell -a &lt;shell command&gt; --become\n# e.g.\n# &gt; ansible -i ./inventory -l &lt;groupname&gt; -m ansible.builtin.shell -a \"apt upgrade -y\" --become\n</code></pre>"},{"location":"ansible/#with-task_1","title":"With task","text":"<pre><code>task ansible:cmd group='kubernetes' cmd='apt upgrade -y' -- '--become'\n</code></pre>"},{"location":"node-prep/","title":"\ud83d\udcbb Machine Preparation","text":""},{"location":"node-prep/#system-requirements","title":"System requirements","text":"<p>\ud83d\udccd k3s default behaviour is that all nodes are able to run workloads, including control nodes. Worker nodes are therefore optional.</p> <p>\ud83d\udccd If you have 3 or more nodes it is strongly recommended to make 3 of them control nodes for a highly available control plane.</p> <p>\ud83d\udccd Ideally you will run the cluster on bare metal machines.</p> Role Cores Memory System Disk Control 4 (6*) 8GB (24GB*) 100GB (500GB*) SSD/NVMe Worker 4 (6*) 8GB (24GB*) 100GB (500GB*) SSD/NVMe * recommended"},{"location":"node-prep/#debian-for-amd64","title":"Debian for AMD64","text":"<ol> <li>Download the latest stable release of Debian, then follow this guide to get it installed.</li> </ol> <p>Deviations from the guide:</p> <p><code>txt Choose \"Guided - use entire disk\" Choose \"All files in one partition\" Delete Swap partition Uncheck all Debian desktop environment options</code></p> <ol> <li>[Post install] Remove CD/DVD as apt source</li> </ol> <p><code>sh    su -    sed -i '/deb cdrom/d' /etc/apt/sources.list    apt update    exit</code></p> <ol> <li>[Post install] Enable sudo for your non-root user</li> </ol> <p><code>sh    su -    apt update    apt install -y sudo    usermod -aG sudo ${username}    echo \"${username} ALL=(ALL) NOPASSWD:ALL\" | tee /etc/sudoers.d/${username}    exit    newgrp sudo    sudo apt update</code></p> <ol> <li>[Post install] Add SSH keys (or use <code>ssh-copy-id</code> on the client that is connecting)</li> </ol> <p>a. Add with <code>ssh-copy-id</code></p> <p><code>sh    ssh-copy-id -i ~/.ssh/id_ed25519 &lt;user&gt;@&lt;host&gt;</code></p> <p>b. Add with github</p> <p>\ud83d\udccd First make sure your ssh keys are up-to-date and added to your github account as instructed.</p> <p><code>sh    mkdir -m 700 ~/.ssh    sudo apt install -y curl    curl https://github.com/${github_username}.keys &gt; ~/.ssh/authorized_keys    chmod 600 ~/.ssh/authorized_keys</code></p>"},{"location":"nosleep-remote/","title":"Preventing Sleep on Debian When Network Connections Are Active","text":"<p>When running remote servers or SSH-accessible systems, you don't want your linux (Debian) machine to suspend while you're connected or performing remote tasks. This approach blocks system suspend whenever an SSH session is open \u2014 without requiring root, sudo, or system-wide configuration.</p> <p>It is designed for:</p> <ul> <li>Headless servers</li> <li>Homelab nodes</li> <li>Remote-admin systems</li> </ul>"},{"location":"nosleep-remote/#what-this-does","title":"What This Does","text":"<p>When you log in via SSH:</p> <ul> <li> <p>A background process registers a sleep inhibitor with <code>systemd-logind</code></p> </li> <li> <p>The inhibitor remains active for the lifetime of the SSH session</p> </li> <li> <p>While the inhibitor is active:</p> </li> <li> <p>Idle suspend is prevented</p> </li> <li> <p>Manual suspend requests are delayed/refused</p> </li> <li> <p>When the SSH session ends:</p> </li> <li> <p>The inhibitor exits automatically</p> </li> <li>Suspend behavior returns to normal</li> </ul>"},{"location":"nosleep-remote/#1-set-polkit-policy-to-skip-authentication-requirement","title":"1. Set Polkit policy to skip authentication requirement","text":"<p>By default, using <code>systemd-inhibit --mode=block</code> requires authentication. This is a frustrating experience and can conflict with other on-login hooks (like neofetch/fastfetch) in interactive shells. We can set a policy to skip the authentication requirement specifically for specific actions associated with <code>ssh-inhibit</code>.</p> <ol> <li>Create policy</li> </ol> <p><code>sh    sudo nano /etc/polkit-1/rules.d/49-ssh-inhibit.rules</code></p> <ol> <li>Paste the rule definition</li> </ol> <p><code>sh    polkit.addRule(function(action, subject) {        if (            subject.active === true &amp;&amp;            subject.isInGroup(\"ssh-inhibit\") &amp;&amp;            (                action.id === \"org.freedesktop.login1.inhibit-block-sleep\" ||                action.id === \"org.freedesktop.login1.inhibit-delay-sleep\" ||                action.id === \"org.freedesktop.login1.inhibit-block-idle\"            )        ) {            return polkit.Result.YES;        }    });</code></p> <ol> <li>Apply the rule</li> </ol> <p><code>sh    sudo chown root:root /etc/polkit-1/rules.d/49-ssh-inhibit.rules    sudo chmod 644 /etc/polkit-1/rules.d/49-ssh-inhibit.rules    sudo systemctl restart polkit</code></p>"},{"location":"nosleep-remote/#2-add-the-ssh-login-hook-to-inhibit-sleep","title":"2. Add the SSH login hook to inhibit sleep","text":"<ol> <li>Edit <code>/etc/ssh/sshrc</code></li> </ol> <p><code>sh    sudo nano /etc/ssh/sshrc</code></p> <ol> <li>Add this block near the end:</li> </ol> <p><code>sh    # Prevent sleep while SSH session is active    systemd-inhibit \\      --what=sleep \\      --mode=block \\      --who=\"SSH session\" \\      --why=\"Active SSH connection\" \\      sleep infinity \\      &lt;/dev/null &gt;/dev/null 2&gt;&amp;1 &amp;</code></p> <p>This inhibits sleep during an SSH connection by blocking sleep:</p> <ul> <li> <p><code>sleep infinity</code>      Keeps the inhibitor alive for the duration of the session</p> </li> <li> <p>`</p> </li> </ul>"},{"location":"observability-review/","title":"Observability review and migration notes","text":""},{"location":"observability-review/#current-layout-this-repository","title":"Current layout (this repository)","text":"<ul> <li>Observability workloads live under <code>kubernetes/apps/observability/</code>, with Grafana, kube-prometheus-stack, VictoriaLogs, Fluent Bit, Karma, Goldilocks, node-problem-detector, nut-exporter, Scrutiny, and speedtest-exporter grouped together in a single namespace tree. Dashboards are bundled with Grafana rather than scoped per-app.</li> <li>Grafana comes from the upstream Helm chart at <code>kubernetes/apps/observability/grafana/app/helmrelease.yaml</code>; datasources for Alertmanager and Prometheus plus dashboard providers/folders (default, Ceph, Flux, Kubernetes, Nginx, Prometheus) are defined directly in values, together with OAuth and plugin settings. Dashboards are pulled from Grafana.com and monitoring-mixins URLs in the same values block, so ownership is centralized inside the HelmRelease.</li> <li>VictoriaLogs is deployed through the bjw-s app-template at <code>kubernetes/apps/observability/victoria-logs/app/helmrelease.yaml</code>, with 10d retention and a Ceph-backed PVC exposed via Envoy. The chart also enables bundled dashboards tied to the existing Grafana datasource UID.</li> <li>kube-prometheus-stack (at <code>kubernetes/apps/observability/kube-prometheus-stack/</code>) supplies Prometheus/Alertmanager and exporters. Grafana is disabled there in favor of the standalone HelmRelease above, and custom rules/relabeling live inside the stack values.</li> </ul>"},{"location":"observability-review/#differences-vs-grafana-operator-based-approach","title":"Differences vs Grafana Operator-based approach","text":"<ul> <li>Repos like onedr0p/home-ops and Diaoul/home-ops use the Grafana Operator to materialize <code>Grafana</code>/<code>GrafanaDatasource</code>/<code>GrafanaDashboard</code> resources. Dashboards typically live next to the owning app (e.g., <code>kubernetes/apps/&lt;namespace&gt;/&lt;app&gt;/</code>), while Grafana core settings stay in the Grafana app directory. This contrasts with the current centralization inside a single HelmRelease values file.</li> <li>Operator-managed folders and datasources are decoupled from Helm values, making it easier to give each app directory its own dashboards while sharing common datasources. Your current setup keeps all dashboard JSON references in one place and uses Helm-managed folders; migrating means splitting these definitions into CRs without losing the existing folder and datasource names.</li> </ul>"},{"location":"observability-review/#migration-suggestions-preserve-existing-tuning","title":"Migration suggestions (preserve existing tuning)","text":"<ol> <li>Introduce Grafana Operator alongside Helm</li> <li>Add the operator to <code>kubernetes/apps/observability/grafana/</code> and mirror current Grafana settings in <code>Grafana</code>/<code>GrafanaDatasource</code> CRs (keep the <code>Alertmanager</code> and <code>Prometheus</code> datasources with the same UIDs and URLs). Preserve OAuth config, plugins, replicas, and storage choices already present in the HelmRelease values.</li> <li>Relocate dashboards with preserved folders</li> <li>Convert the HelmRelease dashboard list into <code>GrafanaDashboard</code> CRs. Place shared/infra dashboards in the Grafana app directory and co-locate app-specific dashboards (Ceph, Flux, VolSync, Kubernetes, Nginx, Prometheus) with their respective app manifests so ownership matches onedr0p/Diaoul patterns. Use folder annotations or <code>GrafanaFolder</code> CRs to keep the existing folder structure intact.</li> <li>Integrate monitoring.mixins.dev assets</li> <li>Render mixins into dashboard and rule manifests and store them with their owners (e.g., cert-manager dashboards in the cert-manager app tree, kube-state-metrics in kube-prometheus-stack). Ensure datasource variables map to your existing aliases (<code>Prometheus</code>, <code>alertmanager</code>) so no rule or dashboard needs to change targets.</li> <li>KEDA, VictoriaLogs, silence-operator</li> <li>Add KEDA as its own app with CRDs managed by Flux; point the metrics scraper at the current Prometheus endpoint and reuse your resource limits/requests to match existing sizing.</li> <li>Keep VictoriaLogs values (10d retention, Ceph PVC, Envoy route) unchanged; if the operator ships dashboards, register them against the same datasource UID used today to avoid breaking queries.</li> <li>Introduce silence-operator targeting the existing Alertmanager route; migrate silences declaratively while leaving kube-prometheus-stack rules intact.</li> <li>Transition plan</li> <li>Run Grafana Operator in parallel, fronted by a temporary host/path until dashboards and datasources reconcile. Once CR-based dashboards match the Helm-managed set, cut traffic over and retire the Helm-managed dashboard provisioning. Maintain current resource settings during the cutover to avoid incidental performance changes.</li> </ol>"},{"location":"onepassword-eso-migration-tooling/","title":"1Password ESO migration tooling","text":"<p>This doc describes the migration tooling used to move SOPS-managed Kubernetes secrets into 1Password for External Secrets Operator (ESO).</p> <p>Workflow:</p> <ol> <li>Crawl SOPS files into metadata inventory.</li> <li>Push decrypted values to 1Password items with <code>op</code>.</li> <li>Generate <code>ExternalSecret</code> manifests that map item fields to K8s secret keys.</li> </ol>"},{"location":"onepassword-eso-migration-tooling/#goals","title":"Goals","text":"<ul> <li>Translate SOPS-managed Kubernetes Secrets into 1Password item fields.</li> <li>Keep plaintext out of disk by decrypting only during the push step.</li> <li>Persist an inventory that can be reused to generate ExternalSecrets.</li> </ul>"},{"location":"onepassword-eso-migration-tooling/#requirements","title":"Requirements","text":"<ul> <li><code>sops</code> installed locally.</li> <li>1Password account and dedicated vault (for example, <code>homelab</code>).</li> <li>1Password CLI (<code>op</code>) installed and authenticated.</li> <li><code>OP_VAULT</code> set in the environment (or pass <code>--vault</code>).</li> </ul>"},{"location":"onepassword-eso-migration-tooling/#inventory-format","title":"Inventory format","text":"<p>The crawler writes <code>inventory.json</code> to a chosen output directory. It is metadata-only and does not store plaintext values.</p> <p>Example:</p> <pre><code>{\n  \"version\": 1,\n  \"root\": \"kubernetes/apps\",\n  \"vault\": \"homelab\",\n  \"entries\": [\n    {\n      \"sops_path\": \"kubernetes/apps/default/homebox/app/secret.sops.yaml\",\n      \"namespace\": \"default\",\n      \"app\": \"homebox\",\n      \"purpose\": \"app\",\n      \"section\": null,\n      \"item_name\": \"default.homebox\",\n      \"fields\": [\n        \"HBOX_MAILER_USERNAME\",\n        \"HBOX_MAILER_PASSWORD\"\n      ],\n      \"item_id\": null,\n      \"ks_path\": \"kubernetes/apps/default/homebox/ks.yaml\",\n      \"helmrelease_path\": \"kubernetes/apps/default/homebox/app/helmrelease.yaml\"\n    }\n  ]\n}\n</code></pre>"},{"location":"onepassword-eso-migration-tooling/#crawl-workflow","title":"Crawl workflow","text":"<p>Crawl a directory and build the inventory:</p> <pre><code>./scripts/onepassword/crawl.py \\\n  --dir kubernetes/apps \\\n  --output-dir ./migration-out \\\n  --vault homelab\n</code></pre> <p>Notes:</p> <ul> <li>Namespace/app inference uses nearest <code>ks.yaml</code> and <code>helmrelease.yaml</code>.</li> <li><code>op</code> references follow <code>op://homelab/&lt;namespace&gt;.&lt;appname&gt;/[section]/&lt;field&gt;</code>.</li> <li>Item titles should be <code>{namespace}.{appname}</code> and section should represent purpose when needed.</li> <li>The crawler stores this as <code>section</code> metadata (<code>null</code> for <code>purpose=app</code>).</li> <li>Separator preference between namespace and app name: <code>.</code>, then <code>_</code>, then <code>-</code>.</li> <li><code>--output-dir</code> defaults to <code>--dir</code>; <code>OP_VAULT</code> can provide the default vault.</li> </ul>"},{"location":"onepassword-eso-migration-tooling/#push-workflow","title":"Push workflow","text":"<p>Push inventory entries to 1Password using <code>op</code>:</p> <pre><code>./scripts/onepassword/push.py \\\n  --inventory ./migration-out/inventory.json\n</code></pre> <p>Notes:</p> <ul> <li>The script decrypts each SOPS file locally and sends item templates to <code>op</code> via stdin.</li> <li>Entries sharing the same <code>item_name</code> are merged into one 1Password item.</li> <li>Entries with a non-null <code>section</code> are written into that section on the item.</li> <li>If two source files map to the same <code>item_name</code> and contain different values for the same field label, that item is skipped and reported as a conflict.</li> <li>ESO's 1Password provider resolves by field label and ignores section names, so field labels must stay unique within an item.</li> <li>Existing item titles prompt for apply mode: per-item, apply-all, or skip-all.</li> <li>Item IDs are written back to the inventory unless <code>--no-write-inventory</code> is used.</li> </ul>"},{"location":"onepassword-eso-migration-tooling/#externalsecret-generation","title":"ExternalSecret generation","text":"<p>Generate <code>ExternalSecret</code> manifests next to each SOPS file:</p> <pre><code>./scripts/onepassword/externalsecrets.py \\\n  --inventory ./migration-out/inventory.json\n</code></pre> <p>Notes:</p> <ul> <li>Output files are derived from SOPS filename, e.g.   <code>secret-oidc.sops.yaml</code> -&gt; <code>secret-oidc.externalsecret.yaml</code>.</li> <li>Generated manifests target <code>ClusterSecretStore</code> named <code>onepassword</code>.</li> <li>For each key, <code>remoteRef.key</code> is the item title and <code>remoteRef.property</code> is the field label.</li> </ul>"},{"location":"onepassword-eso-migration/","title":"1Password Connect migration plan (ESO)","text":"<p>This document outlines the migration from SOPS-managed application secrets to 1Password Connect with External Secrets Operator (ESO).</p>"},{"location":"onepassword-eso-migration/#principles-and-constraints","title":"Principles and constraints","text":"<ul> <li>Keep a minimal SOPS-encrypted bootstrap for cluster bring-up and Flux decryption.</li> <li>Use ESO with a shared <code>ClusterSecretStore</code> named <code>onepassword</code>.</li> <li>Deploy ESO and 1Password Connect in <code>external-secrets</code> namespace.</li> <li>Use a push-style migration: load secrets into 1Password first, then switch workloads to <code>ExternalSecret</code>.</li> <li>Organize cutovers namespace-by-namespace to reduce coordination overhead.</li> </ul>"},{"location":"onepassword-eso-migration/#phase-0-discovery-and-design","title":"Phase 0: Discovery and design","text":"<ul> <li>Inventory current SOPS secrets by namespace and classify shared vs app-specific values.</li> <li>Confirm 1Password vault strategy (default vault: <code>homelab</code>).</li> <li>Confirm naming conventions and field mapping.</li> <li>Prepare migration inputs: SOPS path -&gt; 1Password item title/field labels.</li> </ul>"},{"location":"onepassword-eso-migration/#phase-0-outputs-current-repository-state","title":"Phase 0 outputs (current repository state)","text":"Namespace App SOPS file path Notes bootstrap bootstrap <code>kubernetes/bootstrap/age-key.sops.yaml</code> Shared bootstrap (age key). bootstrap bootstrap <code>kubernetes/bootstrap/github-deploy-key.sops.yaml</code> Shared bootstrap (Flux deploy key). cert-manager cert-manager <code>kubernetes/apps/cert-manager/cert-manager/issuers/secret.sops.yaml</code> App-specific issuer credentials. database cloudnative-pg <code>kubernetes/apps/database/cloudnative-pg/cluster/secrets.sops.yaml</code> App-specific database credentials. database ext-postgres-operator <code>kubernetes/apps/database/ext-postgres-operator/app/secret.sops.yaml</code> Operator credentials. database pgadmin <code>kubernetes/apps/database/pgadmin/app/secret.sops.yaml</code> App-specific credentials. datasci cloudnative-pg <code>kubernetes/apps/datasci/postgres/cloudnative-pg/cluster/secrets.sops.yaml</code> App-specific database credentials. datasci ext-postgres-operator <code>kubernetes/apps/datasci/postgres/ext-postgres-operator/app/secret.sops.yaml</code> Operator credentials. datasci mlflow <code>kubernetes/apps/datasci/mlflow/app/secret.sops.yaml</code> App-specific credentials. datasci mlflow <code>kubernetes/apps/datasci/mlflow/app/secret-gateway.sops.yaml</code> Gateway-specific credentials. datasci mlflow <code>kubernetes/apps/datasci/mlflow/app/secret-s3.sops.yaml</code> Object storage credentials. default calibre-web-automated <code>kubernetes/apps/default/calibre-web-automated/app/secret.sops.yaml</code> App-specific credentials. default calibre-web-automated <code>kubernetes/apps/default/calibre-web-automated/app/secret-oidc.sops.yaml</code> OIDC client credentials. default homebox <code>kubernetes/apps/default/homebox/app/secret.sops.yaml</code> App-specific credentials. default homepage <code>kubernetes/apps/default/homepage/app/secret.sops.yaml</code> App-specific credentials. default jelu <code>kubernetes/apps/default/jelu/app/secret-oidc.sops.yaml</code> OIDC client credentials. default karakeep <code>kubernetes/apps/default/karakeep/app/secret.sops.yaml</code> App-specific credentials. default karakeep <code>kubernetes/apps/default/karakeep/app/secret-oidc.sops.yaml</code> OIDC client credentials. default mealie <code>kubernetes/apps/default/mealie/app/secret.sops.yaml</code> App-specific credentials. default mealie <code>kubernetes/apps/default/mealie/app/secret-oidc.sops.yaml</code> OIDC client credentials. default memos <code>kubernetes/apps/default/memos/app/secret-oidc.sops.yaml</code> OIDC client credentials. default miniflux <code>kubernetes/apps/default/miniflux/app/secret.sops.yaml</code> App-specific credentials. default miniflux <code>kubernetes/apps/default/miniflux/app/secret-oidc.sops.yaml</code> OIDC client credentials. default picoshare <code>kubernetes/apps/default/picoshare/app/secret.sops.yaml</code> App-specific credentials. debug whoami <code>kubernetes/apps/debug/whoami/app/secret-oidc.sops.yaml</code> OIDC client credentials. flux flux vars <code>kubernetes/flux/vars/cluster-secrets.sops.yaml</code> Shared bootstrap variables. flux flux vars <code>kubernetes/flux/vars/custom-secrets.sops.yaml</code> Shared customization variables. flux-system flux-system <code>kubernetes/apps/flux-system/addons/webhooks/github/secret.sops.yaml</code> GitHub webhook secret. network cloudflared <code>kubernetes/apps/network/cloudflared/app/secret.sops.yaml</code> Tunnel credentials. network external-dns <code>kubernetes/apps/network/external-dns/app/secret.sops.yaml</code> DNS provider credentials. observability grafana operator <code>kubernetes/apps/observability/grafana/operator/secret.sops.yaml</code> App-specific credentials. observability grafana operator <code>kubernetes/apps/observability/grafana/operator/secret-oidc.sops.yaml</code> OIDC client credentials. observability kube-prometheus-stack <code>kubernetes/apps/observability/kube-prometheus-stack/app/secret-additionalScrapeConfigs.sops.yaml</code> Additional scrape configs. observability kube-prometheus-stack <code>kubernetes/apps/observability/kube-prometheus-stack/app/secret-alertmgrNotifiers.sops.yaml</code> Alertmanager notifiers. observability nut-exporter <code>kubernetes/apps/observability/nut-exporter/app/secret.sops.yaml</code> App-specific credentials. security authelia <code>kubernetes/apps/security/authelia/app/secret.sops.yaml</code> App-specific credentials. security lldap <code>kubernetes/apps/security/lldap/app/secret.sops.yaml</code> Directory credentials. blog hugo <code>kubernetes/apps/blog/hugo/app/secret.sops.yaml</code> App-specific credentials. <p>Notes:</p> <ul> <li>Bootstrap SOPS files remain encrypted after cutover for bootstrap/Flux workflows.</li> <li>Namespace inventories should drive <code>ExternalSecret</code> mappings during Phase 3.</li> </ul>"},{"location":"onepassword-eso-migration/#phase-1-gitops-foundation-in-kubernetesappsexternal-secrets","title":"Phase 1: GitOps foundation in <code>kubernetes/apps/external-secrets</code>","text":"<p>Status checklist:</p> <ul> <li>[x] Deploy ESO via <code>external-secrets-operator</code> Flux Kustomization.</li> <li>[x] Deploy 1Password Connect + <code>ClusterSecretStore/onepassword</code> via <code>onepassword</code> Flux Kustomization.</li> <li>[x] Configure shared <code>ClusterExternalSecret</code> for <code>cluster-secrets</code> fan-out.</li> <li>[x] Provide example <code>ExternalSecret</code> and <code>ClusterExternalSecret</code> manifests.</li> </ul>"},{"location":"onepassword-eso-migration/#phase-1-outputs-current-repository-state","title":"Phase 1 outputs (current repository state)","text":"<ul> <li>Namespace + app root kustomization: <code>kubernetes/apps/external-secrets/</code>.</li> <li>ESO app: <code>kubernetes/apps/external-secrets/external-secrets-operator/</code>.</li> <li>1Password app: <code>kubernetes/apps/external-secrets/onepassword/</code>.</li> <li><code>ClusterSecretStore</code>: <code>onepassword</code>.</li> <li>Bootstrap secret expected in <code>external-secrets</code> namespace: <code>onepassword-secret</code> with keys:</li> <li><code>1password-credentials.json</code></li> <li><code>token</code></li> </ul>"},{"location":"onepassword-eso-migration/#phase-2-migration-tooling-and-documentation","title":"Phase 2: Migration tooling and documentation","text":"<ul> <li>[x] Inventory crawler: <code>scripts/onepassword/crawl.py</code>.</li> <li>[x] Push helper (SOPS -&gt; 1Password via <code>op</code>): <code>scripts/onepassword/push.py</code>.</li> <li>[x] ExternalSecret generator: <code>scripts/onepassword/externalsecrets.py</code>.</li> <li>[x] Tooling guide: <code>docs/onepassword-eso-migration-tooling.md</code>.</li> </ul>"},{"location":"onepassword-eso-migration/#phase-3-namespace-by-namespace-cutover","title":"Phase 3: Namespace-by-namespace cutover","text":"<ul> <li>[ ] For each namespace:</li> <li>Create/adjust <code>ExternalSecret</code> resources mapping K8s keys to 1Password fields.</li> <li>Update namespace app manifests to consume ESO-managed secrets.</li> <li>Validate reconciliation (<code>ExternalSecret</code>/<code>ClusterSecretStore</code> Ready conditions).</li> <li>Remove direct SOPS consumption once cutover is verified.</li> </ul>"},{"location":"onepassword-eso-migration/#phase-4-cleanup-and-legacy-handling","title":"Phase 4: Cleanup and legacy handling","text":"<ul> <li>[ ] Keep minimal SOPS bootstrap only.</li> <li>[ ] Remove unused SOPS app secrets after successful namespace cutovers.</li> <li>[ ] Add monitoring/alerting for ESO sync errors and store readiness.</li> </ul>"},{"location":"onepassword-eso-migration/#additional-recommended-milestones","title":"Additional recommended milestones","text":"<ul> <li>RBAC hardening: minimize ESO permissions and limit <code>ClusterExternalSecret</code> use where possible.</li> <li>Reliability: set/verify resource requests and disruption budgets for ESO + Connect.</li> <li>Observability: add dashboards/alerts for ESO reconciliation failures.</li> <li>Resilience drills: practice 1Password credential rotation and ESO recovery.</li> </ul>"},{"location":"onepassword-eso-migration/#naming-conventions-1password-eso","title":"Naming conventions (1Password + ESO)","text":"<ul> <li>1Password vault: <code>homelab</code> (use consistently for ESO and <code>op</code> references).</li> <li><code>op</code> reference pattern: <code>op://homelab/&lt;namespace&gt;.&lt;appname&gt;/[section]/&lt;field&gt;</code>.</li> <li>1Password item title: <code>{namespace}.{appname}</code> (example: <code>default.ghost</code>).</li> <li>Use <code>section</code> for grouping by purpose (for example <code>app</code>, <code>oidc</code>, <code>db</code>) when needed.</li> <li>Separator preference between namespace and app name: <code>.</code>, then <code>_</code>, then <code>-</code>.</li> <li>ESO's 1Password provider matches by field label and ignores section names, so field labels must be unique per item.</li> <li>Shared item titles: <code>shared.{purpose}</code>.</li> <li>Secret fields: <code>username</code>, <code>password</code>, <code>apiKey</code>, <code>token</code>, <code>certificate</code>, <code>privateKey</code>.</li> <li>Kubernetes Secret names: <code>app</code> or <code>app-&lt;purpose&gt;</code> in the app namespace.</li> <li>ExternalSecret resource names: <code>{namespace}-{app}-{source}</code> (example: <code>default-ghost</code>).</li> <li>ClusterSecretStore name: <code>onepassword</code>.</li> <li><code>remoteRef.key</code> maps to item title; <code>remoteRef.property</code> maps to field label.</li> </ul>"},{"location":"renovate/","title":"Renovate Policy and Requirements","text":"<p>This document defines how Renovate is expected to operate in this repository. It is both an introduction and a normative specification.</p>"},{"location":"renovate/#what-renovate-does-here","title":"What Renovate Does Here","text":"<ul> <li>Detects dependency references in Kubernetes, Ansible, and Taskfile YAML.</li> <li>Creates update PRs with consistent metadata (labels, semantic commits, dashboard tracking).</li> <li>Applies safety controls (release-age delay and merge-confidence signals).</li> <li>Auto-merges only explicitly trusted low-risk updates.</li> <li>Supports custom dependency extraction for non-standard version fields.</li> </ul>"},{"location":"renovate/#requirement-language","title":"Requirement Language","text":"<ul> <li><code>MUST</code>: required behavior.</li> <li><code>SHOULD</code>: strong default; deviations require clear justification in PR context.</li> <li><code>MAY</code>: optional behavior.</li> </ul>"},{"location":"renovate/#manager-and-scope-requirements","title":"Manager and Scope Requirements","text":""},{"location":"renovate/#kubernetes-manager","title":"<code>kubernetes</code> manager","text":"<ul> <li>MUST scan YAML in <code>.taskfiles/</code>, <code>ansible/</code>, and <code>kubernetes/</code>.</li> <li>MUST detect container image references in Kubernetes-style manifests and related YAML.</li> <li>SHOULD be the default manager for infrastructure YAML unless a narrower manager is more correct.</li> </ul>"},{"location":"renovate/#flux-manager","title":"<code>flux</code> manager","text":"<ul> <li>MUST scan YAML in <code>kubernetes/</code> for Flux-related dependency references.</li> <li>SHOULD be used to keep Flux-source and Flux-managed references current without manual tag edits.</li> </ul>"},{"location":"renovate/#helm-values-manager","title":"<code>helm-values</code> manager","text":"<ul> <li>MUST scan YAML in <code>kubernetes/</code> for Helm values image/version references.</li> <li>SHOULD be used to update chart values files that do not appear as direct <code>HelmRelease</code> version fields.</li> </ul>"},{"location":"renovate/#custom-regex-managers","title":"Custom regex managers","text":"<ul> <li>MUST exist for dependency patterns not handled correctly by built-in managers.</li> <li>MUST constrain <code>managerFilePatterns</code> to repo paths that actually contain the target pattern.</li> <li>SHOULD prefer path-specific <code>managerFilePatterns</code> (for example <code>*source*.yaml</code>, <code>*helmrelease*.yaml</code>, <code>*grafanadashboard*.yaml</code>) over repo-wide Kubernetes YAML globs to reduce no-op extraction work.</li> <li>MAY use constrained token matching in filenames (prefix/suffix around a canonical token) when resource names vary by app conventions.</li> <li>MUST include a deterministic <code>datasourceTemplate</code>.</li> <li>SHOULD include explicit versioning templates when upstream tags are non-standard.</li> <li>MUST include enough context in regex to avoid accidental cross-line or unrelated matches.</li> <li>MUST support these repository use cases:</li> <li>Annotated <code># renovate:</code> dependency declarations in YAML.</li> <li>OCI refs of the form <code>oci://repo:tag</code>.</li> <li>CloudnativePG PostgreSQL image tags.</li> <li>Cloudflare cache tag values tied to Hugo image versions.</li> </ul>"},{"location":"renovate/#safety-and-stability-requirements","title":"Safety and Stability Requirements","text":"<ul> <li>Renovate MUST default PR creation delay to <code>3 days</code> to reduce churn from immediately-retracted releases and improve merge-confidence signal quality.</li> <li>Exceptions to the global delay MAY be used only for explicitly documented, allowlist-scoped rules.</li> <li>Renovate MUST handle missing/partial release timestamps for GHCR images via <code>minimumReleaseAgeBehaviour=timestamp-optional</code>.</li> <li>Renovate MUST ignore encrypted and generated paths that should never be dependency-churn surfaces:</li> <li><code>**/*.sops.*</code></li> <li><code>**/.archive/**</code></li> <li><code>**/resources/**</code></li> <li><code>.copier-answers.yaml</code></li> <li>Renovate MUST pin container images matching <code>postgresql</code> to <code>&lt;17</code>.</li> </ul>"},{"location":"renovate/#update-and-merge-policy-requirements","title":"Update and Merge Policy Requirements","text":"<ul> <li>PRs MUST be separated by update type (major/minor/patch/digest) to improve review clarity.</li> <li>PRs SHOULD group tightly-coupled ecosystem components (for example Flux, Cilium, Cert-Manager) to reduce noisy single-package PR floods.</li> <li>Auto-merge MUST be allowlist-based and scoped by datasource, package matchers, and update type.</li> <li>Auto-merge SHOULD target low-risk updates first:</li> <li>digest updates for trusted container publishers</li> <li>minor/patch/digest updates for trusted pre-commit tooling</li> <li>curated GitHub Actions and selected release streams</li> <li>GitHub Actions auto-merge MUST use two lanes:</li> <li>General lane: all <code>github-actions</code> updates for <code>minor</code>/<code>patch</code>/<code>digest</code> with <code>minimumReleaseAge=3 days</code> and <code>ignoreTests=false</code>.</li> <li>Trusted fast lane: explicitly allowlisted packages only, <code>minor</code>/<code>patch</code>/<code>digest</code>, with <code>minimumReleaseAge=1 minute</code>.</li> <li>Trusted fast lane package matching MUST use exact package names (no org-wide regex wildcards).</li> <li>Current trusted fast lane package allowlist:</li> <li><code>actions/create-github-app-token</code></li> <li><code>actions/checkout</code></li> <li><code>actions/labeler</code></li> <li><code>renovatebot/github-action</code></li> <li>Personal blog container updates for <code>ghcr.io/ahgraber/aimlbling-about</code> MAY auto-merge immediately (<code>minimumReleaseAge=null</code>) across <code>major</code>/<code>minor</code>/<code>patch</code>/<code>digest</code> to minimize CVE exposure.</li> <li>Major updates SHOULD default to review-required unless a package has explicit high-confidence policy coverage.</li> </ul>"},{"location":"renovate/#pr-metadata-requirements","title":"PR Metadata Requirements","text":"<ul> <li>Renovate PRs MUST include one update-type label: <code>type/major</code>, <code>type/minor</code>, <code>type/patch</code>, or <code>type/digest</code>.</li> <li>Renovate PRs SHOULD include datasource/manager labels when applicable (container, helm, github-action, github-release, ansible, copier, pip).</li> <li>Commit messages MUST follow semantic commit conventions and encode update intent:</li> <li>major -&gt; breaking semantic form</li> <li>minor -&gt; feature semantic form</li> <li>patch -&gt; fix semantic form</li> <li>digest -&gt; chore semantic form</li> </ul>"},{"location":"renovate/#versioning-requirements","title":"Versioning Requirements","text":"<ul> <li>Non-standard upstream version formats MUST define explicit <code>versioning</code> rules.</li> <li>Custom versioning SHOULD be documented with a one-line rationale in the same PR that introduces it.</li> <li>Known non-semver ecosystems in this repo (for example <code>k3s</code> release format) MUST use dedicated regex versioning.</li> </ul>"},{"location":"renovate/#configuration-layout-rules","title":"Configuration Layout Rules","text":"<p>This section defines where new policy should go. It does not require current files to be renamed.</p> <ul> <li>Root Renovate config (<code>.renovaterc.json5</code>) MUST stay minimal and composition-focused:</li> <li>global toggles</li> <li>manager enablement/scope</li> <li>inclusion of local preset modules</li> <li>Local preset modules under <code>.renovate/</code> SHOULD be organized by concern:</li> <li>merge policy (<code>automerge</code>, confidence, release-age overrides)</li> <li>extraction logic (<code>customManagers</code>, custom datasources)</li> <li>update shaping (<code>groups</code>, separation strategy)</li> <li>metadata (<code>labels</code>, semantic commit templates)</li> <li>constraints (<code>allowedVersions</code>, versioning overrides)</li> <li>A single preset module SHOULD own a concern; avoid duplicating the same matcher across modules unless intentionally layered.</li> <li>Any new preset module MUST be documented in this file under the relevant requirement section, not as a file inventory dump.</li> </ul>"},{"location":"renovate/#change-control-requirements","title":"Change Control Requirements","text":"<p>When Renovate policy changes:</p> <ol> <li>The same PR MUST update this document with the new/changed requirement.</li> <li>The PR description SHOULD explain risk impact (noise, safety, merge velocity, or breakage risk).</li> <li>Policy changes MUST be validated with Renovate tooling in CI or local dev shell before merge.</li> </ol>"},{"location":"task/","title":"Task","text":"<p>Task is a task runner / build tool that aims to be simple and easy to use.</p>"},{"location":"task/#use","title":"Use","text":"<p>Once the binary is installed, tasks can be listed with:</p> <pre><code>task --list\n</code></pre> <p>To get a summary of the task args and use:</p> <pre><code>task &lt;namespace&gt;:&lt;taskname&gt; --summary\n</code></pre>"},{"location":"task/#variables","title":"Variables","text":"<p>Variables can be provided in tasks with <code>&lt;name&gt;=&lt;value&gt;</code> notation:</p> <pre><code>task &lt;namespace&gt;:&lt;taskname&gt; var='value'\n</code></pre> <p>If allowed, additional arbitrary strings (i.e., cli flags, etc) can be passed following a <code>--</code>:</p> <pre><code>task &lt;namespace&gt;:&lt;taskname&gt; var='value' -- '--quiet'\n</code></pre>"}]}