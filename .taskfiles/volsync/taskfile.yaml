---
version: "3"

x-task-vars: &task-vars
  app: '{{ .app }}'
  controller: '{{ .controller }}'
  ns: '{{ .ns }}'
  claim: '{{ .claim }}'
  ts: '{{ .ts }}'
  ks: '{{ .ks }}'
  previous: '{{ .previous }}'

vars:
  destinationTemplate: "{{ .ROOT_DIR }}/.taskfiles/volsync/templates/ReplicationDestination.tmpl.yaml"
  wipeJobTemplate: "{{ .ROOT_DIR }}/.taskfiles/volsync/templates/WipeJob.tmpl.yaml"
  waitForJobScript: "{{ .ROOT_DIR }}/.taskfiles/volsync/scripts/wait-for-job.sh"
  listJobTemplate: "{{ .ROOT_DIR }}/.taskfiles/volsync/templates/ListJob.tmpl.yaml"
  unlockJobTemplate: "{{ .ROOT_DIR }}/.taskfiles/volsync/templates/UnlockJob.tmpl.yaml"
  ts: '{{ now | date "150405" }}'

tasks:

  relpath:
    desc: Calculate relative path from one file to another
    summary: ...
    dir: "{{ .ROOT_DIR }}"
    cmd: echo $(realpath --relative-to=$(dirname "{{ .start }}") "{{ .end }}")
    requires:
      vars: ["start"]
    vars:
      # start:
      end: '{{ .end | default "./kubernetes/templates/volsync" }}'
    preconditions:
      - { msg: "Start file not found", sh: "test -f {{ .start }}" }
      - { msg: "End file not found", sh: "test -d {{ .end }}" }

  # --------------------------------------------------------------------------
  list:
    desc: List all snapshots taken by restic for a given ReplicationSource (ex. task volsync:list app=plex [ns=default])
    silent: true
    cmds:
      - envsubst < <(cat {{ .listJobTemplate }}) | kubectl apply -f -
      - bash {{ .waitForJobScript }} list-{{ .app }}-{{ .ts }} {{ .ns }}
      - kubectl -n {{ .ns }} wait job/list-{{ .app }}-{{ .ts }} --for condition=complete --timeout=1m
      - kubectl -n {{ .ns }} logs job/list-{{ .app }}-{{ .ts }} --container list
      - kubectl -n {{ .ns }} delete job list-{{ .app }}-{{ .ts }}
    vars:
      app: '{{ or .app (fail "ReplicationSource `app` is required") }}'
      ns: '{{ .ns | default "default" }}'
    env: *task-vars
    preconditions:
      - sh: test -f {{ .waitForJobScript }}
      - sh: test -f {{ .listJobTemplate }}

  unlock:
    desc: Unlocks restic repository for a given ReplicationSource (ex. task volsync:unlock app=plex [ns=default])
    silent: true
    cmds:
      - envsubst < <(cat {{ .unlockJobTemplate }}) | kubectl apply -f -
      - bash {{ .waitForJobScript }} unlock-{{ .app }}-{{ .ts }} {{ .ns }}
      - kubectl -n {{ .ns }} wait job/unlock-{{ .app }}-{{ .ts }} --for condition=complete --timeout=1m
      - kubectl -n {{ .ns }} logs job/unlock-{{ .app }}-{{ .ts }} --container unlock
      - kubectl -n {{ .ns }} delete job unlock-{{ .app }}-{{ .ts }}
    vars:
      app: '{{ or .app (fail "ReplicationSource `app` is required") }}'
      ns: '{{ .ns | default "default" }}'
    env: *task-vars
    preconditions:
      - sh: test -f {{ .waitForJobScript }}
      - sh: test -f {{ .unlockJobTemplate }}

  # To run backup jobs in parallel for all replicationsources:
  #  - kubectl get replicationsources --all-namespaces --no-headers | awk '{print $2, $1}' | xargs --max-procs=4 -l bash -c 'task volsync:snapshot app=$0 ns=$1'
  #
  snapshot:
    desc: Trigger a Restic ReplicationSource snapshot (ex. task volsync:snapshot app=plex [ns=default])
    cmds:
      - kubectl -n {{ .ns }} patch replicationsources {{ .app }} --type merge -p '{"spec":{"trigger":{"manual":"{{ .ts }}"}}}'
      - bash {{ .waitForJobScript }} volsync-src-{{ .app }} {{ .ns }}
      - kubectl -n {{ .ns }} wait job/volsync-src-{{ .app }} --for condition=complete --timeout=120m
      # TODO: Find a way to output logs
      # Error from server (NotFound): jobs.batch "volsync-src-zzztest" not found
      # - kubectl -n {{ .ns }} logs job/volsync-src-{{ .app }}
    vars:
      app: '{{ or .app (fail "ReplicationSource `app` is required") }}'
      ns: '{{ .ns | default "default" }}'
    env: *task-vars
    preconditions:
      - sh: test -f {{ .waitForJobScript }}
      - sh: kubectl -n {{ .ns }} get replicationsources {{ .app }}
        msg: "ReplicationSource '{{ .app }}' not found in namespace '{{ .ns }}'"

  # To run restore jobs in parallel for all replicationdestinations:
  #   - kubectl get replicationsources --all-namespaces --no-headers | awk '{print $2, $1}' | xargs --max-procs=2 -l bash -c 'task volsync:restore app=$0 ns=$1'
  #
  restore:
    desc: Trigger a Restic ReplicationSource restore (ex. task volsync:restore app=plex [ns=default])
    cmds:
      - task: suspend-task
        vars: *task-vars
      - task: wipe-task
        vars: *task-vars
      - task: restore-task
        vars: *task-vars
      - task: resume-task
        vars: *task-vars
    vars:
      app: '{{ or .app (fail "Variable `app` is required") }}'
      ns: '{{ .ns | default "default" }}'
      # 1) Query to find the Flux Kustomization associated with the ReplicationSource (app)
      ks:
        sh: |
          kubectl -n {{ .ns }} get replicationsource {{ .app }} \
            -o jsonpath="{.metadata.labels.kustomize\.toolkit\.fluxcd\.io/name}"
      # 2) Query to find the Claim associated with the ReplicationSource (app)
      claim:
        sh: |
          kubectl -n {{ .ns }} get replicationsource {{ .app }} \
            -o jsonpath="{.spec.sourcePVC}"
      # 3) Query to find the controller associated with the PersistentVolumeClaim (claim)
      controller:
        sh: |
          app=$(kubectl -n {{ .ns }} get persistentvolumeclaim {{ .claim }} -o jsonpath="{.metadata.labels.app\.kubernetes\.io/name}")
          if kubectl -n {{ .ns }} get deployment.apps/$app >/dev/null 2>&1 ; then
            echo "deployment.apps/$app"
          else
            echo "statefulset.apps/$app"
          fi
      previous: "{{ .previous | default 2 }}"
    env: *task-vars
    preconditions:
      - sh: test -f {{ .wipeJobTemplate }}
      - sh: test -f {{ .destinationTemplate }}
      - sh: test -f {{ .waitForJobScript }}

  # Suspend the Flux ks and hr
  suspend-task:
    internal: true
    cmds:
      - flux -n flux-system suspend kustomization {{ .ks }}
      - flux -n {{ .ns }} suspend helmrelease {{ .app }}
      - kubectl -n {{ .ns }} scale {{ .controller }} --replicas 0
      - kubectl -n {{ .ns }} wait pod --for delete --selector="app.kubernetes.io/name={{ .app }}" --timeout=2m
    env: *task-vars

  # Wipe the PVC of all data
  wipe-task:
    internal: true
    cmds:
      - envsubst < <(cat {{ .wipeJobTemplate }}) | kubectl apply -f -
      - bash {{ .waitForJobScript }} wipe-{{ .app }}-{{ .claim }}-{{ .ts }} {{ .ns }}
      - kubectl -n {{ .ns }} wait job/wipe-{{ .app }}-{{ .claim }}-{{ .ts }} --for condition=complete --timeout=120m
      - kubectl -n {{ .ns }} logs job/wipe-{{ .app }}-{{ .claim }}-{{ .ts }} --container wipe
      - kubectl -n {{ .ns }} delete job wipe-{{ .app }}-{{ .claim }}-{{ .ts }}
    env: *task-vars

  # Create VolSync replicationdestination CR to restore data
  restore-task:
    internal: true
    cmds:
      - envsubst < <(cat {{ .destinationTemplate }}) | kubectl apply -f -
      - bash {{ .waitForJobScript }} volsync-dst-{{ .app }}-{{ .claim }}-{{ .ts }} {{ .ns }}
      - kubectl -n {{ .ns }} wait job/volsync-dst-{{ .app }}-{{ .claim }}-{{ .ts }} --for condition=complete --timeout=120m
      - kubectl -n {{ .ns }} delete replicationdestination {{ .app }}-{{ .claim }}-{{ .ts }}
    env: *task-vars

  # Resume Flux ks and hr
  resume-task:
    internal: true
    cmds:
      - flux -n {{ .ns }} resume helmrelease {{ .app }}
      - flux -n flux-system resume kustomization {{ .ks }}
    env: *task-vars
