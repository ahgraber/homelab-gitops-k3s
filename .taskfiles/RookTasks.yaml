---
version: "3"

includes:
  ansible:
    taskfile: ./AnsibleTasks.yaml
    internal: true
  cluster:
    taskfile: ./ClusterTasks.yaml
    internal: true

x-task-vars: &task-vars
  namespace: '{{.namespace}}'
  operator: '{{.operator}}'
  cluster: '{{.cluster}}'
  cephcluster: '{{.cephcluster}}'

tasks:

  toolbox:
    desc: Open a shell to debug ceph via the toolbox
    interactive: true
    cmd: kubectl exec -it {{.pod}} -n rook-ceph -- bash
    vars:
      pod:
        sh: kubectl get pod -l "app=rook-ceph-tools" -n rook-ceph -o json | jq '.items[] | .metadata.name'

  dash-pass:
    desc: Retrieve the rook-ceph dashboard password
    cmds:
      - |
        kubectl get secret rook-ceph-dashboard-password -n rook-ceph -o json |
        jq '.data.password' |
        base64 --decode |
        pbcopy
      - echo "Token is in clipboard ready to paste!"

  suspend-ceph:
    desc: Suspend Ceph operator and cluster
    internal: true
    cmds:
      - flux suspend kustomization '{{.operator}}' -n flux-system
      - flux suspend kustomization '{{.cluster}}' -n flux-system
    vars:
      # n: '{{.n | default "rook-ceph"}}'
      operator: '{{.operator | default "rook-ceph-operator"}}'
      cluster: '{{.cluster | default "rook-ceph-cluster"}}'
    env: *task-vars

  scaledown-kps:
    desc: Scale down kube-prometheus-stack (managed by operator)
    internal: true
    cmds:
      - flux suspend hr kube-prometheus-stack -n '{{.namespace}}'
      - |
        kubectl patch Alertmanager '{{.name}}' -n '{{.namespace}}' \
        --type merge --patch='{"spec":{"replicas": 0}}'
      - |
        kubectl patch Prometheus '{{.name}}' -n '{{.namespace}}' \
        --type merge --patch='{"spec":{"replicas": 0}}'
    vars:
      name: kps
      namespace: monitoring

  delete-hr-using-ceph:
    desc: Suspend kustomizations and delete helmreleases with rook-ceph PVCs
    internal: true
    cmds:
      - for: { var: ceph_ks, as: KS, split: "\n" }
        cmd: flux suspend kustomization '{{.KS}}' -n flux-system
      - for: { var: ceph_ks, as: KS, split: "\n" }
        cmd: |
          kubectl get hr --all-namespaces -o json |
          jq -rc --arg ks '{{.KS}}' '.items[] |
            select(.metadata.labels."kustomize.toolkit.fluxcd.io/name"==$ks) |
            [.metadata.name, .metadata.namespace] |
            @tsv' |
          xargs -L1 bash -c 'kubectl delete hr "$0" -n "$1"'
    vars:
      ceph_ks:  # get kustomization names associated with pvcs using 'ceph' storageclasses
        sh: |
          kubectl get pvc --all-namespaces -o json |
          jq -rc '.items[] |
            select(.spec.storageClassName | contains("ceph")) |
            select(.metadata.labels."kustomize.toolkit.fluxcd.io/name"!=null) |
            .metadata.labels."kustomize.toolkit.fluxcd.io/name"
          '
    status:
      - 'test -z "{{.ceph_ks}}"'

  delete-ceph-blockpools:
    desc: Delete ceph blockpools
    internal: true
    cmds:
      - for: { var: blockpools, as: BP, split: "\n" }
        cmd: |
          echo '{{.BP}}' |
          xargs -L1 bash -c 'kubectl delete cephblockpool "$0" -n "$1"'
          # kubectl patch cephblockpool "$0" -n "$1" --type merge --patch='{"metadata":{"finalizers": []}}'
    vars:
      blockpools:
        sh: |
          kubectl get cephblockpool --all-namespaces -o json |
          jq -rc '.items[] |
            [.metadata.name, .metadata.namespace] |
            @tsv
          '
    status:
      - 'test -z "{{.blockpools}}"'

  delete-ceph-filesystems:
    desc: Delete ceph filesystems
    internal: true
    cmds:
      - for: { var: filesystems, as: FS, split: "\n" }
        cmd: |
          echo '{{.FS}}' |
          xargs -L1 bash -c 'kubectl delete cephfilesystem "$0" -n "$1"'
          # kubectl patch cephfilesystem "$0" -n "$1" --type merge --patch='{"metadata":{"finalizers": []}}'
    vars:
      filesystems:
        sh: |
          kubectl get cephfilesystem --all-namespaces -o json |
          jq -rc '.items[] |
            [.metadata.name, .metadata.namespace] |
            @tsv
          '
    status:
      - 'test -z "{{.filesystems}}"'

  delete-ceph-objectstores:
    desc: Delete ceph objectstores
    internal: true
    cmds:
      - for: { var: objectstores, as: OS, split: "\n" }
        cmd: |
          echo '{{.OS}}' |
          xargs -L1 bash -c 'kubectl delete cephobjectstore "$0" -n "$1"'
          # kubectl patch cephobjectstore "$0" -n "$1" --type merge --patch='{"metadata":{"finalizers": []}}'
    vars:
      objectstores:
        sh: |
          kubectl get cephobjectstore --all-namespaces -o json |
          jq -rc '.items[] |
            [.metadata.name, .metadata.namespace] |
            @tsv
          '
    status:
      - 'test -z "{{.objectstores}}"'

  delete-ceph-storageclasses:
    desc: Delete storage classes starting with 'ceph'
    internal: true
    cmds:
      - for: { var: storageclasses, as: SC, split: "\n" }
        cmd: kubectl delete storageclass '{{.SC}}'
    vars:
      storageclasses:
        sh: |
            kubectl get storageclass -o json |
            jq -rc '.items[] |
              select(.metadata.name | contains("ceph")) |
              .metadata.name
            '
    status:
      - 'test -z "{{.storageclasses}}"'

  delete-ceph-pvcs:
    desc: Delete pvcs using 'ceph' storageclasses
    internal: true
    cmds:
      - for: { var: ceph_pvcs, as: PVC, split: "\n" }
        cmd: |
          echo '{{.PVC}}' |
          xargs -L1 bash -c 'kubectl delete pvc "$0" -n "$1"'
    vars:
      ceph_pvcs:  # get pvc names associated with pvcs using 'ceph' storageclasses
        sh: |
          kubectl get pvc --all-namespaces -o json |
          jq -rc '.items[] |
            select(.spec.storageClassName | contains("ceph")) |
            [.metadata.name, .metadata.namespace] |
            @tsv
          '
    status:
      - 'test -z "{{.ceph_pvcs}}"'

  delete-ceph-pvs:
    desc: Delete pvs using 'ceph' storageclasses
    internal: true
    cmds:
      - for: { var: ceph_pvs, as: PV, split: "\n" }
        cmd: |
          # kubectl patch pv '{{.PV}}' --type merge --patch='{"metadata":{"finalizers": []}}'
          kubectl delete pv '{{.PV}}'
    vars:
      ceph_pvs:  # get pv names associated with pvcs using 'ceph' storageclasses
        sh: |
          kubectl get pv --all-namespaces -o json |
          jq -rc '.items[] |
            select(.spec.storageClassName | contains("ceph")) |
            .metadata.name
          '
    status:
      - 'test -z "{{.ceph_pvs}}"'

  delete-ceph-configmaps:
    desc: Patch and delete configmaps in namespace
    internal: true
    cmds:
      - for: { var: configmaps, as: CFG, split: "\n" }
        cmd: |
          # kubectl patch configmap '{{.CFG}}' -n {{.namespace}} --type merge --patch='{"metadata":{"finalizers": []}}'
          kubectl delete configmap '{{.CFG}}' -n {{.namespace}}
    vars:
      namespace: '{{.namespace | default "rook-ceph"}}'
      configmaps:
        sh: |
          kubectl get configmaps -n '{{.namespace}}' -o json |
          jq -rc '.items[] | .metadata.name'
    env: *task-vars
    status:
      - 'test -z "{{.configmaps}}"'

  delete-ceph-secrets:
    desc: Patch and delete secrets in namespace
    internal: true
    cmds:
      - for: { var: secrets, as: S, split: "\n" }
        cmd: |
          # kubectl patch secret '{{.S}}' -n {{.namespace}} --type merge --patch='{"metadata":{"finalizers": []}}'
          kubectl delete secret '{{.S}}' -n {{.namespace}}
    vars:
      namespace: '{{.namespace | default "rook-ceph"}}'
      secrets:
        sh: |
          kubectl get secrets -n '{{.namespace}}' -o json |
          jq -rc '.items[] | .metadata.name'
    env: *task-vars
    status:
      - 'test -z "{{.secrets}}"'

  delete-ceph-cluster:
    desc: Patch and delete cephcluster object
    internal: true
    cmds:
      - for: { var: cephclusters, as: CC, split: "\n" }
        cmd: |
          kubectl patch cephcluster {{.CC}} -n {{.namespace}} \
            --type merge --patch='{"spec":{"cleanupPolicy": {"confirmation":"yes-really-destroy-data"}}}'
          kubectl delete cephcluster {{.CC}} -n {{.namespace}}
          # kubectl patch cephcluster {{.CC}}} -n {{.namespace}} --type merge --patch='{"metadata":{"finalizers": []}}'
    vars:
      namespace: '{{.namespace | default "rook-ceph"}}'
      # cephcluster: '{{.cephcluster | default "rook-ceph"}}'
      cephclusters:
        sh: |
          kubectl get cephclusters -n {{.namespace}} -o json |
          jq -rc '.items[] | .metadata.name'
    env: *task-vars
    status:
      - 'test -z "{{.cephclusters}}"'

  delete-cluster-flux:
    desc: Delete rook-ceph cluster helmrelease and kustomization
    internal: true
    cmds:
      - kubectl delete hr {{.cluster}} -n {{.namespace}} --ignore-not-found
      - kubectl delete kustomization {{.cluster}} -n flux-system --ignore-not-found
    vars:
      namespace: '{{.namespace | default "rook-ceph"}}'
      cluster: '{{.helmrelease | default "rook-ceph-cluster"}}'
    env: *task-vars

  delete-operator-flux:
    desc: Delete rook-ceph operator helmrelease and kustomization
    internal: true
    cmds:
      - kubectl delete hr {{.operator}} -n {{.namespace}} --ignore-not-found
      - kubectl delete kustomization {{.operator}} -n flux-system --ignore-not-found
    vars:
      namespace: '{{.namespace | default "rook-ceph"}}'
      operator: '{{.operator | default "rook-ceph-operator"}}'
    env: *task-vars

  delete-ceph-crds:
    desc: Delete rook-ceph crds
    internal: true
    cmds:
      - for: { var: crds, as: CRD, split: "\n" }
        cmd: |
          # kubectl patch crd '{{.CRD}}' --type merge --patch='{"metadata":{"finalizers": []}}'
          kubectl delete crd '{{.CRD}}'
    vars:
      crds:
        sh: |
          kubectl get crd --all-namespaces -o json |
          jq -rc '.items[] |
            select(.metadata.name | contains("ceph.rook.io")) |
            .metadata.name
          '
    status:
      - 'test -z "{{.crds}}"'

  delete-ceph-ns:
    desc: Delete rook-ceph namespace
    internal: true
    cmds:
      - kubectl delete ns {{.namespace}} --ignore-not-found
      # - |
      #   kubectl patch ns {{.namespace}} --type merge --patch='{"spec":{"finalizers": []}}'
    vars:
      namespace: '{{.namespace | default "rook-ceph"}}'
    env: *task-vars

  # https://rook.io/docs/rook/v1.11/Getting-Started/ceph-teardown/#delete-the-block-and-file-artifacts
  teardown:
    desc: Tear down rook-ceph cluster
    interactive: true
    prompt: Are you sure you want to nuke rook-ceph cluster?
    cmds:
      - flux suspend kustomization apps -n flux-system
      - task: suspend-ceph
        vars: *task-vars
      - task: scaledown-kps
      - task: delete-hr-using-ceph
      - task: delete-ceph-pvcs
      - task: delete-ceph-pvs
      - task: delete-ceph-blockpools
      - task: delete-ceph-filesystems
      - task: delete-ceph-objectstores
      - task: delete-ceph-storageclasses
      - task: delete-ceph-configmaps
        vars: *task-vars
      - task: delete-ceph-secrets
        vars: *task-vars
      - task: delete-ceph-cluster
        vars: *task-vars
      - task: delete-cluster-flux
        vars: *task-vars
      - task: delete-operator-flux
        vars: *task-vars
      - task: delete-ceph-crds
      - task: delete-ceph-ns
        vars: *task-vars
      - echo "!! Don't forget to run rook-ceph cleanup ansible script  --  `task ansible:ceph-cleanup` !!"
      - task: ansible:ceph-cleanup
    vars:
      namespace: '{{.namespace | default "rook-ceph"}}'
      operator: '{{.operator | default "rook-ceph-operator"}}'
      cluster: '{{.cluster | default "rook-ceph-cluster"}}'
      cephcluster: '{{.cephcluster | default "rook-ceph"}}'
    env: *task-vars

  resume-kps:
    desc: Resume kube-prometheus-stack (managed by operator)
    internal: true
    cmds:
      - |
        kubectl patch Alertmanager '{{.name}}' -n '{{.namespace}}' \
        --type merge --patch='{"spec":{"replicas": '{{.replicas}}'}}'
      - |
        kubectl patch Prometheus '{{.name}}' -n '{{.namespace}}' \
        --type merge --patch='{"spec":{"replicas": '{{.replicas}}'}}'
      - flux resume hr kube-prometheus-stack -n monitoring
      - flux resume kustomization monitoring-kube-prometheus-stack -n flux-system
    vars:
      name: kps
      namespace: monitoring
      replicas: 1

  resume:
    desc: Resume applications
    cmds:
      - task: cluster:resume-all-hr
      - task: cluster:resume-all-ks
      - task: resume-kps
      - task: cluster:reconcile
