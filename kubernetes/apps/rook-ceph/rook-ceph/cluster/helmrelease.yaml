---
apiVersion: helm.toolkit.fluxcd.io/v2beta1
kind: HelmRelease
metadata:
  name: rook-ceph-cluster
  namespace: rook-ceph
spec:
  interval: 15m
  chart:
    spec:
      # renovate: registryUrl=https://charts.rook.io/release
      chart: rook-ceph-cluster
      version: v1.10.8
      sourceRef:
        kind: HelmRepository
        name: rook-ceph-charts
        namespace: flux-system
  maxHistory: 3
  install:
    createNamespace: true
    remediation:
      retries: 3
  upgrade:
    cleanupOnFail: true
    remediation:
      retries: 3
  uninstall:
    keepHistory: false
  values:
    annotations:
      all:
        reloader.stakater.com/search: "true"
        # configmap.reloader.stakater.com/reload: "ceph-values-block,ceph-values-filesystem,ceph-values-object"
    monitoring:
      enabled: true
      createPrometheusRules: true
      # rulesNamespaceOverride: monitoring
    toolbox:
      enabled: true
      tolerations:
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
          operator: Exists
      nodeSelector:
        node-role.kubernetes.io/control-plane: "true"
      resources:
        requests:
          cpu: 100m
          memory: 64M
        limits:
          memory: 128M

    configOverride: |
      [global]
      bdev_enable_discard = true
      bdev_async_discard = true

    cephClusterSpec:
      crashCollector:
        disable: false
      dashboard:
        enabled: true
        urlPrefix: /
        ssl: false
      network:
        provider: host
      storage:
        useAllNodes: false
        useAllDevices: false
        config:
          osdsPerDevice: "1"
        nodes:
          - name: optiplex3
            devices:
              - name: "/dev/disk/by-id/ata-SATA_SSD_22010312000052"
          - name: optiplex4
            devices:
              - name: "/dev/disk/by-id/ata-SATA_SSD_22010312000015"
          - name: optiplex5
            devices:
              - name: "/dev/disk/by-id/ata-SATA_SSD_22010312000028"
      resources:
        mgr:
          requests:
            cpu: 125m
            memory: 512Mi
          limits:
            memory: 1Gi
        mon:
          requests:
            cpu: 125m
            memory: 512Mi
          limits:
            memory: 1Gi
        osd:
          requests:
            cpu: 500m
            memory: 256Mi
          limits:
            memory: 2Gi
        prepareosd:
          requests:
            cpu: 250m
            memory: 50M
          limits:
            memory: 256M
        mgr-sidecar:
          requests:
            cpu: 50m
            memory: 94M
          limits:
            memory: 256M
        crashcollector:
          requests:
            cpu: 15m
            memory: 64M
          limits:
            memory: 64M
        logcollector:
          requests:
            cpu: 100m
            memory: 100M
          limits:
            memory: 1Gi
        cleanup:
          requests:
            cpu: 250m
            memory: 100M
          limits:
            memory: 1G

  valuesFrom:
    - kind: ConfigMap
      name: ceph-values-block
    - kind: ConfigMap
      name: ceph-values-filesystem
    - kind: ConfigMap
      name: ceph-values-object

  ### https://raw.githubusercontent.com/rook/rook/v1.10.8/deploy/examples/monitoring/localrules.yaml
  ### ref: https://rook.github.io/docs/rook/v1.9/ceph-monitoring.html#customize-alerts
  postRenderers:
    #  use built-in "kustomize" post renderer.
    - kustomize:
        patches:
          - target:
              group: monitoring.coreos.com
              version: v1
              kind: PrometheusRule
              name: kps-kubernetes-storage
              namespace: monitoring
            patch: |
              # CephNodeNetworkPacketDrops
              - op: replace
                path: /spec/groups/6/rules/1/expr
                value: |
                  (
                    increase(node_network_receive_drop_total{device!="lo"}[1m]) +
                    increase(node_network_transmit_drop_total{device!="lo"}[1m])
                  ) / (
                    increase(node_network_receive_packets_total{device!="lo"}[1m]) +
                    increase(node_network_transmit_packets_total{device!="lo"}[1m])
                  ) >= 0.005 or (
                    increase(node_network_receive_drop_total{device!="lo"}[1m]) +
                    increase(node_network_transmit_drop_total{device!="lo"}[1m])
                  ) >= 75
              - op: add
                path: /spec/groups/6/rules/1/for
                value: 1m

              # # CephNodeNetworkPacketErrors
              # - op: add
              #   path: /spec/groups/6/rules/2/for
              #   value: 1m

              # CephNodeInconsistentMTU
              - op: remove
                path: /spec/groups/6/rules/4

              # # CephPoolGrowthWarning
              # - op: replace
              #   path: /spec/groups/7/rules/0/expr
              #   # value: |
              #   #   (predict_linear(
              #   #     (max(ceph_pool_percent_used) without (pod, instance))[2d:1h], 3600 * 24 * 5
              #   #   ) * on(pool_id)
              #   value: |
              #     (predict_linear(ceph_pool_percent_used[2d], 3600 * 24 * 5) * on(pool_id)
              #     group_right ceph_pool_metadata) >= 95"
