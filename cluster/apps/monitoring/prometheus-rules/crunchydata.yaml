---
# GENERATED FILE - DO NOT EDIT
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: crunchydata-rules
  namespace: postgres
spec:
  groups:
    # - name: rules/aggregation.rules
    #   rules:
    #     # - expr: sum without(store) (capacity{job="cockroachdb"})
    #     #   record: node:capacity

    - name: rules/alerts.rules
      rules:
        ########## EXPORTER RULES ##########
        - alert: PGExporterScrapeError
          expr: pg_exporter_last_scrape_error > 0
          for: 60s
          labels:
            service: postgresql
            severity: critical
          annotations:
            summary: "Postgres Exporter running on {{ $labels.job }} (instance: {{ $labels.instance }}) is encountering scrape errors processing queries. Error count: ( {{ $value }} )"
        ########## POSTGRESQL RULES ##########
        - alert: PGIsUp
          expr: pg_up < 1
          for: 60s
          labels:
            service: postgresql
            severity: critical
          annotations:
            summary: "postgres_exporter running on {{ $labels.job }} is unable to communicate with the configured database"

        # - alert: PGIdleTxn
        #   expr: ccp_connection_stats_max_idle_in_txn_time > 300
        #   for: 60s
        #   labels:
        #     service: postgresql
        #     severity: warning
        #   annotations:
        #     description: '{{ $labels.job }} has at least one session idle in transaction for over 5 minutes.'
        #     summary: 'PGSQL Instance idle transactions'
        # - alert: PGIdleTxn
        #   expr: ccp_connection_stats_max_idle_in_txn_time > 900
        #   for: 60s
        #   labels:
        #     service: postgresql
        #     severity: critical
        #   annotations:
        #     description: '{{ $labels.job }} has at least one session idle in transaction for over 15 minutes.'
        #     summary: 'PGSQL Instance idle transactions'
        - alert: PGQueryTime
          expr: ccp_connection_stats_max_query_time > 43200
          for: 60s
          labels:
            service: postgresql
            severity: warning
          annotations:
            description: "{{ $labels.job }} has at least one query running for over 12 hours."
            summary: "PGSQL Max Query Runtime"
        - alert: PGQueryTime
          expr: ccp_connection_stats_max_query_time > 86400
          for: 60s
          labels:
            service: postgresql
            severity: critical
          annotations:
            description: "{{ $labels.job }} has at least one query running for over 1 day."
            summary: "PGSQL Max Query Runtime"
        - alert: PGConnPerc
          expr: 100 * (ccp_connection_stats_total / ccp_connection_stats_max_connections) > 75
          for: 60s
          labels:
            service: postgresql
            severity: warning
          annotations:
            description: "{{ $labels.job }} is using 75% or more of available connections ({{ $value }}%)"
            summary: "PGSQL Instance connections"
        - alert: PGConnPerc
          expr: 100 * (ccp_connection_stats_total / ccp_connection_stats_max_connections) > 90
          for: 60s
          labels:
            service: postgresql
            severity: critical
          annotations:
            description: "{{ $labels.job }} is using 90% or more of available connections ({{ $value }}%)"
            summary: "PGSQL Instance connections"
        - alert: PGDiskSize
          expr: 100 * ((ccp_nodemx_data_disk_total_bytes - ccp_nodemx_data_disk_available_bytes) / ccp_nodemx_data_disk_total_bytes) > 75
          for: 60s
          labels:
            service: postgresql
            severity: warning
          annotations:
            description: 'PGSQL Instance {{ $labels.deployment }} over 75% disk usage at mount point "{{ $labels.mount_point }}": {{ $value }}%'
            summary: PGSQL Instance usage warning
        - alert: PGDiskSize
          expr: 100 * ((ccp_nodemx_data_disk_total_bytes - ccp_nodemx_data_disk_available_bytes) / ccp_nodemx_data_disk_total_bytes) > 90
          for: 60s
          labels:
            service: postgresql
            severity: critical
          annotations:
            description: 'PGSQL Instance {{ $labels.deployment }} over 90% disk usage at mount point "{{ $labels.mount_point }}": {{ $value }}%'
            summary: "PGSQL Instance size critical"
        - alert: PGReplicationByteLag
          expr: ccp_replication_status_byte_lag > 5.24288e+07
          for: 60s
          labels:
            service: postgresql
            severity: warning
          annotations:
            description: "PGSQL Instance {{ $labels.job }} has at least one replica lagging over 50MB behind."
            summary: "PGSQL Instance replica lag warning"
        - alert: PGReplicationByteLag
          expr: ccp_replication_status_byte_lag > 1.048576e+08
          for: 60s
          labels:
            service: postgresql
            severity: critical
          annotations:
            description: "PGSQL Instance {{ $labels.job }} has at least one replica lagging over 100MB behind."
            summary: "PGSQL Instance replica lag warning"
        - alert: PGReplicationSlotsInactive
          expr: ccp_replication_slots_active == 0
          for: 60s
          labels:
            service: postgresql
            severity: critical
          annotations:
            description: "PGSQL Instance {{ $labels.job }} has one or more inactive replication slots"
            summary: "PGSQL Instance inactive replication slot"
        - alert: PGXIDWraparound
          expr: ccp_transaction_wraparound_percent_towards_wraparound > 50
          for: 60s
          labels:
            service: postgresql
            severity: warning
          annotations:
            description: "PGSQL Instance {{ $labels.job }} is over 50% towards transaction id wraparound."
            summary: "PGSQL Instance {{ $labels.job }} transaction id wraparound imminent"
        - alert: PGXIDWraparound
          expr: ccp_transaction_wraparound_percent_towards_wraparound > 75
          for: 60s
          labels:
            service: postgresql
            severity: critical
          annotations:
            description: "PGSQL Instance {{ $labels.job }} is over 75% towards transaction id wraparound."
            summary: "PGSQL Instance transaction id wraparound imminent"
        - alert: PGEmergencyVacuum
          expr: ccp_transaction_wraparound_percent_towards_emergency_autovac > 110
          for: 60s
          labels:
            service: postgresql
            severity: warning
          annotations:
            description: "PGSQL Instance {{ $labels.job }} is over 110% beyond autovacuum_freeze_max_age value. Autovacuum may need tuning to better keep up."
            summary: "PGSQL Instance emergency vacuum imminent"
        - alert: PGEmergencyVacuum
          expr: ccp_transaction_wraparound_percent_towards_emergency_autovac > 125
          for: 60s
          labels:
            service: postgresql
            severity: critical
          annotations:
            description: "PGSQL Instance {{ $labels.job }} is over 125% beyond autovacuum_freeze_max_age value. Autovacuum needs tuning to better keep up."
            summary: "PGSQL Instance emergency vacuum imminent"
        - alert: PGArchiveCommandStatus
          expr: ccp_archive_command_status_seconds_since_last_fail > 300
          for: 60s
          labels:
            service: postgresql
            severity: critical
          annotations:
            description: "PGSQL Instance {{ $labels.job }} has a recent failing archive command"
            summary: "Seconds since the last recorded failure of the archive_command"
        - alert: PGSequenceExhaustion
          expr: ccp_sequence_exhaustion_count > 0
          for: 60s
          labels:
            service: postgresql
            severity: critical
          annotations:
            description:
              "Count of sequences on instance {{ $labels.job }} at over 75% usage: {{ $value }}. Run following query to see full sequence status: SELECT * FROM monitor.sequence_status() WHERE percent
              >= 75"
        - alert: PGSettingsPendingRestart
          expr: ccp_settings_pending_restart_count > 0
          for: 60s
          labels:
            service: postgresql
            severity: critical
          annotations:
            description:
              "One or more settings in the pg_settings system catalog on system {{ $labels.job }} are in a pending_restart state. Check the system catalog for which settings are pending and review
              postgresql.conf for changes."

      ########## PGBACKREST RULES ##########
      # Uncomment and customize one or more of these rules to monitor your pgbackrest backups.
      # Full backups are considered the equivalent of both differentials and incrementals since both are based on the last full
      #   And differentials are considered incrementals since incrementals will be based off the last diff if one exists
      #   This avoid false alerts, for example when you don't run diff/incr backups on the days that you run a full
      # Stanza should also be set if different intervals are expected for each stanza.
      #   Otherwise rule will be applied to all stanzas returned on target system if not set.
      #
      # Relevant metric names are:
      #   ccp_backrest_last_full_time_since_completion_seconds
      #   ccp_backrest_last_incr_time_since_completion_seconds
      #   ccp_backrest_last_diff_time_since_completion_seconds
      #
      #  - alert: PGBackRestLastCompletedFull_main
      #    expr: ccp_backrest_last_full_backup_time_since_completion_seconds{stanza="main"} > 604800
      #    for: 60s
      #    labels:
      #       service: postgresql
      #       severity: critical
      #    annotations:
      #       summary: 'Full backup for stanza [main] on system {{ $labels.job }} has not completed in the last week.'
      #
      #  - alert: PGBackRestLastCompletedIncr_main
      #    expr: ccp_backrest_last_incr_backup_time_since_completion_seconds{stanza="main"} > 86400
      #    for: 60s
      #    labels:
      #       service: postgresql
      #       severity: critical
      #    annotations:
      #       summary: 'Incremental backup for stanza [main] on system {{ $labels.job }} has not completed in the last 24 hours.'
      #
      #
      # Runtime monitoring is handled with a single metric:
      #
      #   ccp_backrest_last_runtime_backup_runtime_seconds
      #
      # Runtime monitoring should have the "backup_type" label set.
      #   Otherwise the rule will apply to the last run of all backup types returned (full, diff, incr)
      # Stanza should also be set if runtimes per stanza have different expected times
      #
      #  - alert: PGBackRestLastRuntimeFull_main
      #    expr: ccp_backrest_last_runtime_backup_runtime_seconds{backup_type="full", stanza="main"} > 14400
      #    for: 60s
      #    labels:
      #       service: postgresql
      #       severity: critical
      #    annotations:
      #       summary: 'Expected runtime of full backup for stanza [main] has exceeded 4 hours'
      #
      #  - alert: PGBackRestLastRuntimeDiff_main
      #    expr: ccp_backrest_last_runtime_backup_runtime_seconds{backup_type="diff", stanza="main"} > 3600
      #    for: 60s
      #    labels:
      #       service: postgresql
      #       severity: critical
      #    annotations:
      #       summary: 'Expected runtime of diff backup for stanza [main] has exceeded 1 hour'
      ##
      #
      ## If the pgbackrest command fails to run, the metric disappears from the exporter output and the alert never fires.
      ## An absence alert must be configured explicitly for each target (job) that backups are being monitored.
      ## Checking for absence of just the full backup type should be sufficient (no need for diff/incr).
      ## Note that while the backrest check command failing will likely also cause a scrape error alert, the addition of this
      ## check gives a clearer answer as to what is causing it and that something is wrong with the backups.
      #
      #  - alert: PGBackrestAbsentFull_Prod
      #    expr: absent(ccp_backrest_last_full_backup_time_since_completion_seconds{job="Prod"})
      #    for: 10s
      #    labels:
      #      service: postgresql
      #      severity: critical
      #    annotations:
      #      description: 'Backup Full status missing for Prod. Check that pgbackrest info command is working on target system.'
