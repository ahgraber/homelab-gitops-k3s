---
apiVersion: v1
kind: ConfigMap
metadata:
  name: values-prometheus
  namespace: monitoring
data:
  values.yaml: |
    ## Deploy a Prometheus instance
    prometheus:
      enabled: true

      nameOverride: prometheus

      ingress:
        enabled: false # see ingressRoute

      ## Settings affecting prometheusSpec
      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#prometheusspec
      prometheusSpec:
        ## If true, pass --storage.tsdb.max-block-duration=2h to prometheus. This is already done if using Thanos
        disableCompaction: false
        ## Interval between consecutive scrapes. Default 30s.
        scrapeInterval: ""
        ## Number of seconds to wait for target to respond before erroring
        scrapeTimeout: ""
        ## EnableAdminAPI enables Prometheus the administrative HTTP API which includes functionality such as deleting time series.
        enableAdminAPI: true

        ## External URL at which Prometheus will be reachable.
        externalUrl: "prometheus.${SECRET_DOMAIN}"

        tolerations: []
        nodeSelector: {}
        podAntiAffinity: soft
        podAntiAffinityTopologyKey: kubernetes.io/hostname

        ## Number of replicas of each shard to deploy for a Prometheus deployment.
        ## Number of replicas multiplied by shards is the total number of Pods created.
        replicas: 1
        ## Name of the external label used to denote replica name
        replicaExternalLabelName: replica

        ## EXPERIMENTAL: Number of shards to distribute targets onto.
        ## Number of replicas multiplied by shards is the total number of Pods created.
        ## Note that scaling down shards will not reshard data onto remaining instances, it must be manually moved.
        ## Increasing shards will not reshard data either but it will continue to be available from the same instances.
        ## To query globally use Thanos sidecar and Thanos querier or remote write data to a central location.
        ## Sharding is done on the content of the `__address__` target meta-label.
        shards: 1

        ## How long to retain metrics
        retention: 36h # 3d
        ## Maximum size of metrics
        retentionSize: 3GiB
        ## Enable compression of the write-ahead log using Snappy.
        walCompression: true

        ## Log level for Prometheus be configured in
        logLevel: info

        ## Resource limits & requests
        resources:
          requests:
            cpu: 500m
            memory: 6Gi
          limits:
            cpu: 750m
            memory: 7.5Gi

        ## Prometheus StorageSpec for persistent data
        ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/user-guides/storage.md
        storageSpec:
          volumeClaimTemplate:
            spec:
              storageClassName: local-path # csi-truenas-iscsi
              resources:
                requests:
                  storage: 10Gi
              ## for attaching retained PV
              # selector:
              #   matchLabels:
              #     app.kubernetes.io/instance: kps-prometheus
              #     app.kubernetes.io/name: prometheus

        ## Alertmanagers to which alerts will be sent
        ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#alertmanagerendpoints
        alertingEndpoints:
          - name: alertmanager
            namespace: monitoring
            port: http
            # scheme: http
            # pathPrefix: ""
            # tlsConfig: {}
            # bearerTokenFile: ""
            # apiVersion: v2

        ## Secrets is a list of Secret [names?] in the same namespace as the Prometheus object, to be mounted into the Prometheus Pods.
        ## The Secrets are mounted into /etc/prometheus/secrets/.
        ## Secrets changes after initial creation of a Prometheus object are not
        ## reflected in the running Pods. To change the secrets mounted into the Prometheus Pods,
        ## the object must be deleted and recreated with the new list of secrets.
        secrets: []

        ## ConfigMaps is a list of ConfigMap [names?] in the same namespace as the Prometheus object, to be mounted into the Prometheus Pods.
        ## The ConfigMaps are mounted into /etc/prometheus/configmaps/.
        configMaps: []


        ## If true, a nil or {} value for prometheus.prometheusSpec.___Selector will cause the
        ## prometheus resource to be created with selectors based on values in the helm deployment,
        ## which will also match the Prometheus___resources created
        ruleSelectorNilUsesHelmValues: false
        serviceMonitorSelectorNilUsesHelmValues: false
        podMonitorSelectorNilUsesHelmValues: false
        probeSelectorNilUsesHelmValues: false

        ## AdditionalScrapeConfigs allows specifying additional Prometheus scrape configurations.
        ## Scrape configurations are appended to the configurations generated by the Prometheus Operator.
        ## Job configurations must have the form as specified in the official Prometheus documentation:
        ## https://prometheus.io/docs/prometheus/latest/configuration/configuration/#scrape_config.
        ## As scrape configs are appended, the user is responsible to make sure it is valid.
        ## Note that using this feature may expose the possibility to break upgrades of Prometheus.
        ## It is advised to review Prometheus release notes to ensure that no incompatible scrape configs are going to break Prometheus after the upgrade.
        ## AdditionalScrapeConfigs can be defined as a list or as a templated string.
        additionalScrapeConfigs:
          - job_name: opnsense
            honor_timestamps: true
            static_configs:
              - targets:
                  - "opnsense.${SECRET_DOMAIN}:9100"
          - job_name: truenas-minio
            bearer_token: eyJhbGciOiJIUzUxMiIsInR5cCI6IkpXVCJ9.eyJleHAiOjQ4MDUxODEzODIsImlzcyI6InByb21ldGhldXMiLCJzdWIiOiJadGdROWdHcmJ3RUNaOGt4WlJRbyJ9.W1oCrRnCbP3SJCGiT1I5yrfAYqhO4coehQqWUduF8DLXKhOStS3LteNrPCPn5fodjWHydu_BILwkc6AhOwWe9g
            metrics_path: /minio/v2/metrics/node
            scheme: https
            static_configs:
              - targets:
                  - "truenas.${SECRET_DOMAIN}:9000"
          - job_name: crunchy-postgres-exporter
            kubernetes_sd_configs:
              - role: pod
            relabel_configs:
              - source_labels: [__meta_kubernetes_pod_label_postgres_operator_crunchydata_com_crunchy_postgres_exporter,__meta_kubernetes_pod_label_crunchy_postgres_exporter]
                action: keep
                regex: true
                separator: ""
              - source_labels: [__meta_kubernetes_pod_container_port_number]
                action: drop
                regex: 5432
              - source_labels: [__meta_kubernetes_pod_container_port_number]
                action: drop
                regex: 10000
              - source_labels: [__meta_kubernetes_pod_container_port_number]
                action: drop
                regex: 8009
              - source_labels: [__meta_kubernetes_pod_container_port_number]
                action: drop
                regex: 2022
              - source_labels: [__meta_kubernetes_pod_container_port_number]
                action: drop
                regex: ^$
              - source_labels: [__meta_kubernetes_namespace]
                action: replace
                target_label: kubernetes_namespace
              - source_labels: [__meta_kubernetes_pod_name]
                target_label: pod
              - source_labels: [__meta_kubernetes_pod_label_postgres_operator_crunchydata_com_cluster,__meta_kubernetes_pod_label_pg_cluster]
                target_label: cluster
                separator: ""
                replacement: '$1'
              - source_labels: [__meta_kubernetes_namespace,cluster]
                target_label: pg_cluster
                separator: ":"
                replacement: '$1$2'
              - source_labels: [__meta_kubernetes_pod_ip]
                target_label: ip
                replacement: '$1'
              - source_labels: [__meta_kubernetes_pod_label_postgres_operator_crunchydata_com_instance,__meta_kubernetes_pod_label_deployment_name]
                target_label: deployment
                replacement: '$1'
                separator: ""
              - source_labels: [__meta_kubernetes_pod_label_postgres_operator_crunchydata_com_role,__meta_kubernetes_pod_label_role]
                target_label: role
                replacement: '$1'
                separator: ""
              - source_labels: [dbname]
                target_label: dbname
                replacement: '$1'
              - source_labels: [relname]
                target_label: relname
                replacement: '$1'
              - source_labels: [schemaname]
                target_label: schemaname
                replacement: '$1'

        ## If additional scrape configurations are already deployed in a single secret file you can use this section.
        ## additionalScrapeConfigsSecret cannot be used with additionalScrapeConfigs
        ## Expected values are the secret name and key
        additionalScrapeConfigsSecret: {}
          # enabled: false
          # name:
          # key:

        ## prometheus.prometheusSpec.thanos allows configuring various aspects of a Prometheus server in a Thanos environment.
        ## This section is experimental, it may change significantly without deprecation notice in any release.
        ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#thanosspec
        thanos: {}
          # secretProviderClass:
          #   provider: gcp
          #   parameters:
          #     secrets: |
          #       - resourceName: "projects/$PROJECT_ID/secrets/testsecret/versions/latest"
          #         fileName: "objstore.yaml"
          # objectStorageConfigFile: /var/secrets/object-store.yaml


      # ## Thanos sidecar
      # ## Thanos service discovery on sidecar
      # ## (Please remember to change ${kube-prometheus-stack.fullname} and ${namespace}. Not just copy and paste!)
      # thanosService:
      #   enabled: true
      # ## ServiceMonitor to scrape Sidecar metrics
      # ## Needs thanosService to be enabled as well
      # thanosServiceMonitor:
      #   enabled: true

      # ## Service for external access to sidecar
      # ## Enabling this creates a service to expose thanos-sidecar outside the cluster.
      # thanosServiceExternal:
      #   enabled: false

      # ## Ingress exposes thanos sidecar outside the cluster
      # thanosIngress:
      #   enabled: false # use ingressRoute if required
